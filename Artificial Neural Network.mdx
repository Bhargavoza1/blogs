---
title: 'Artificial Neural Network'
image: 'https://raw.githubusercontent.com/Bhargavoza1/blogs/main/images/ann/neuron.png'
description: 'A neural network is like a team of interconnected perceptrons (basic computing units) that imitates, in a simplified way, how the human brain works...'
date: '2023-12-14'
tags: ['deeplearning','neuralnetwork','ann','blog']
---

I have always been fascinate by the workings of the human brain and how it enables us to comprehend various sounds and understand the details of colors. 
With each new piece of knowledge I acquire, I realize that there are numerous concepts at play in our quest for understanding.

The human brain is very complex and advanced organ, and its ability to process information and perceive details is a result of complex neural mechanisms. However, what is the fundamental concept that underlies these processes? 
Mathematics and wave functions emerge as key players in explaining and comprehending the mechanisms of perception and information processing within the brain. In this blog, we will solely focus on the role of Mathematics in this regard.

Now, let's delve into a fascinating realm that bridges the sophistication of the human brain with cutting-edge technology—artificial neural networks.


# Neuron
<CustomImage src="https://raw.githubusercontent.com/Bhargavoza1/blogs/main/images/ann/neuron.png"  alt="biological neuron" height={512} width={1000} />

## Biological Neuron
The human brain consists of a complex network of billions of interconnected neurons. Neurons play a crucial role in processing and transmitting chemical and electrical signals. 
Cell nucleus(core) or Soma processes the information received from dendrites. Axon is a cable that is used by neurons to send information. Synapse is the connection between an axon and other neuron dendrites. 
Dendrites, on the other hand, are extensions that receive information from neighboring neurons. 
his brief statement provides us short idea of how our neuron cell behave.

## Artificial Neuron(Perceptron)
Artificial neuron, also referred to as perceptrons, serve as the fundamental building blocks of Artificial neural networks. 
These mathematical functions are derived from the structure of biological neurons and can be likened to basic logic gates that produce binary outputs.  

<br/><br/>
# Perceptron
It is the primary step to learn Machine Learning and Deep Learning technologies, which consists of a set of weights, input values or scores, and a bias. 
Perceptron is a building block of an Artificial Neural Network. Initially, in the mid of 19th century, Mr. Frank Rosenblatt invented the Perceptron for performing certain calculations to detect input data capabilities or business intelligence. 
Perceptron is a linear Machine Learning algorithm used for supervised learning for various binary classifiers. This algorithm enables neurons to learn elements and processes them one by one during preparation. 
<CustomImage src="https://raw.githubusercontent.com/Bhargavoza1/blogs/main/images/ann/perceptron.png"  alt="perceptron" height={512} width={512} />

Please be patient as I provide a detailed explanation of neural networks. We will thoroughly dissect each component of the neural network, and I will also provide a Python code snippet for the mathematical equations involved.
Let’s begin 


## Input Layer(X)
The Perceptron's main element is responsible for receiving the initial data and facilitating its subsequent processing within the system. Each input node is designed to hold a specific real numerical value.

## Weight(W)
The weight parameter signifies the magnitude of the link between units. It is a crucial parameter within the Perceptron components. The weight is directly correlated to the influence of the corresponding input neuron in determining the output. 
In other words, The weights in an ANN are crucial because they directly influence the signals transmitted across the network and ultimately determine the network's output. 
During the training phase, the ANN learns by iteratively adjusting these weights to predict the correct output for a given set of inputs. 
The set of weights in the network encapsulates what the network has learned from the training data, effectively encoding the knowledge necessary for making predictions or decisions.

## Weighted sum
In the first step first, multiply all input values with corresponding weight values and then add them to determine the weighted sum. Mathematically, we can calculate the weighted sum as follows:

```math
\sum_{i=0}^n W_iX_i
```

```Python
import numpy as np

inputs = np.array([-4,0,2,-5])
weights = np.array([0.1,0.5,0.7,1.2])

weightsum= np.dot(inputs,weights)


# weightsum= dot(x,w)
# 5 =  dot([0,0,2,3] , [0.1,0.5,0.7,1.2])
#-5 =  dot([-4,0,2,-5] , [0.1,0.5,0.7,1.2])

```

## Activation Function
An activation function is a mathematical operation applied to the input of a neuron in a neural network. It determines the output of the neuron, which is then used as input for the next layer in the network. 
Activation functions introduce non-linearity to the network, allowing it to learn and approximate complex relationships in data. but in single perceptron we have linear decision boundaries. 

In the context of a single perceptron, the step function is often used as the activation function. Is a mathematical function that produces a binary output based on whether the input exceeds a specified threshold. 
The output is typically 1 if the input is greater than or equal to the threshold and 0 otherwise.

For more information about activations function, please visit this given Wikipedia link. -> [Wikipedia](https://en.wikipedia.org/wiki/Activation_function)
or this blog -> [v7labs](https://www.v7labs.com/blog/neural-networks-activation-functions)

```math  
Y = 
\begin{cases} 
1, & \text{if} \quad \sum^n_{i=0} W_i X_i \geq 0 \\ 
0, & \text{else} 
\end{cases}
```
```Python
def step_function(weightsum):
    if(weightsum >= 0):
        return 1
    else:
        return 0

step_function(weightsum)

# weightsum= dot(x,w)

# 5 =  dot([0,0,2,3] , [0.1,0.5,0.7,1.2])
# 1 = step_function(5)

#-5 =  dot([-4,0,2,-5]  , [0.1,0.5,0.7,1.2])
# 0 = step_function(-5)
```

## Bias
In neural network , bias refers to a constant that is added to the summation of product of features and weights. Its purpose is to adjust the outcome and assist the models in shifting the activation function towards either the positive or negative side. 
However, in the case of a single perceptron with a step function, the practical significance of the bias is limited. It becomes more significant in more complex neural network architectures where non-linear activation functions are used and the network needs to learn and adapt to various patterns in the data.

However, I will make an attempt to explain this with help of code.

 ```math
Y = 
\begin{cases} 
1, & \text{if} \quad \sum^n_{i=0} W_i X_i + b \geq 0 \\ 
0, & \text{else} 
\end{cases}
```

```Python
import numpy as np

inputs = np.array([0,0,2,3])
weights = np.array([0.1,0.5,0.7,1.2])
bias = -6

weightsum= np.dot(inputs,weights) + bias

def step_function(weightsum):
    if(weightsum >= 0):
        return 1
    else:
        return 0

step_function(weightsum)

# weightsum= dot(x,w)

#bias = -6
# -1 =  dot([0,0,2,3] , [0.1,0.5,0.7,1.2]) + bias
# 0 = step_function(-1)

# bias = 6
# 1 =  dot([-4,0,2,-5] , [0.1,0.5,0.7,1.2]) + bias
# 1 = step_function(1)
```
This showcases how the bias term influences the decision boundary of the perceptron. Adjusting the bias allows the perceptron to shift its decision threshold, influencing whether it activates or not based on the input features.

## Limitations of single perceptron
 - Binary Outputs
 - Inability to Learn XOR Function
 - Sensitivity to Input Changes
 - Not Suitable for Nonlinear Problems
 - Difficulty in Training
 - Single-Layer Limitation

The development of multilayer perceptrons (MLPs) was a direct response to the limitations mentioned above. These MLPs were designed specifically to address and overcome these challenges.
MLP also know as Neural Network.

<br/><br/>
# Neural Network
A neural network is like a team of interconnected perceptrons (basic computing units) that imitates, in a simplified way, how the human brain works. 
Each perceptron is connected to others, and during learning, these connections get adjusted. While it's inspired by the brain,It's a simpler model created for specific tasks, not as complex as the real thing.

## Multi-Layer Perceptron
A Multi-Layer Perceptron (MLP) is a type of artificial neural network that consists of multiple layers of nodes, or neurons(Perceptrons). 
<CustomImage src="https://raw.githubusercontent.com/Bhargavoza1/blogs/main/images/ann/MLP.png"  alt="MLP" height={512} width={1000} />

 **Note:** Forward propagation and backward propagation are terms commonly used in the context of artificial neural networks, which are a key component of ML/DL models. These processes are fundamental to training neural networks.

## Forward propagation
Forward propagation, also known as forward pass or inference, is the process in which input data is passed through a neural network to generate predictions or outputs. 
It involves the sequential computation of the network's activations from the input layer to the output layer. The key steps in forward propagation are as follows:
 - **Input Layer:** This layer consists of nodes that represent the input features of the model. Each node corresponds to a feature of the input data.
 - **Hidden Layers:** Between the input and output layers, there can be one or more hidden layers. Each hidden layer contains nodes that perform computations on the input data. These layers are responsible for learning patterns and representations from the input.
 - **Output Layer:** The output layer produces the final result of the network's computation. The number of nodes in the output layer depends on the type of problem the MLP is designed to solve. For example, in a binary classification problem, there might be one node for each class.


### Let's break the Forward propagation

equation to calculat forward pass
We can compute this formula for every output neuron in one shot using a dot product :

<br/>
```math  
X= \begin{bmatrix}
x_{0} & ... & x_{i}\\
 \end{bmatrix}  
```
```math 
W= \begin{bmatrix}
w_{00} & ... & w_{0j}\\
. & . & .\\
. &  . & .\\
w_{i0} & ... & w_{ij}\end{bmatrix}  
 ```
 ```math 
B= \begin{bmatrix}
b_{0} & ... & b_{j}\\
 \end{bmatrix}
```
<br/>
```math  
y_j = \sum_{i=0,j=0}^n x_i.w_{ij} + b_j 
```

module.py (base class for all layers)
```python  
class Module:
    def __init__(self):
        self.input = None
        self.output = None

    def forward_pass(self, input):
        raise NotImplementedError
```

linear.py (implement our equation)
```python  
from module import Module
import numpy as np


class Linear(Module):
    def __init__(self, in_features: int, out_features: int, bias: bool = True, w_b_range: float = 0.5):
        # Initialize weights with random values between -w_b_range and w_b_range
        self.weights = np.random.rand(in_features, out_features) - w_b_range

        # Initialize bias if bias is True, otherwise set bias to 0.0
        self.bias = np.random.rand(1, out_features) - w_b_range if bias else 0.0

    def forward(self, input_features: int):
        self.input_features = input_features
        self.out_features = np.dot(self.input_features, self.weights) + self.bias
        return self.out_features
```

These are activation functions i am using in this code.
  - Sigmoid
    - Equation: $$f(x) =  \frac{\ {1} }{\ {1} + e^{-x} }$$
    - Image 
<CustomImage src="https://raw.githubusercontent.com/Bhargavoza1/blogs/main/images/ann/activation/sigmoid.png"  alt="MLP" height={512} width={512} />
  - Relu
    - Equation: $$f(x) =  \max(0,x)$$
    - Image
<CustomImage src="https://raw.githubusercontent.com/Bhargavoza1/blogs/main/images/ann/activation/relu.png"  alt="MLP" height={512} width={512} />
  - Tenh
    - Equation: $$f(x) =  \frac{\ e^{x} - e^{-x} } {\ e^{x} + e^{-x} } $$
    - Image
<CustomImage src="https://raw.githubusercontent.com/Bhargavoza1/blogs/main/images/ann/activation/tanh.png"  alt="MLP" height={512} width={512} />
 
 
 
<br/>
```math  
y_j = {Activation}\left({ \sum_{i=0,j=0}^n x_i.w_{ij} + b_j }\right)
```

activation.py  
```python  
from module import Module
import numpy as np

class ReLU(Module):
    def __init__(self):
        # Constructor, but it's empty in this case
        pass

    def forward(self, input_features):
        # Forward pass of the ReLU activation function
        # Save the input features for potential use in backward pass
        self.input_features = input_features

        # Apply ReLU activation element-wise
        self.activation = np.maximum(0, self.input_features)

        # Return the result of the activation
        return self.activation


class Sigmoid(Module):
    def __init__(self):
        # Constructor, but it's empty in this case
        pass

    def sigmoid(self, x):
        return 1 / (1 + np.exp(-x))

    def forward(self, input_features):
        # Forward pass of the sigmoid activation function
        # Save the input features for potential use in the backward pass
        self.input_features = input_features

        # Apply the sigmoid activation element-wise
        self.activation = self.sigmoid(self.input_features)

        # Return the result of the activation
        return self.activation


class Tanh(Module):
    def __init__(self):
        super().__init__()

    def tanh(self, x):
        return np.tanh(x)

    def forward(self, input_features):
        # Forward pass of the tanh activation function
        # Save the input features for potential use in the backward pass
        self.input_features = input_features

        # Apply the tanh activation element-wise
        self.activation = self.tanh(self.input_features)

        # Return the result of the activation
        return self.activation


class BinaryStep(Module):
    def __init__(self):
        pass

    def binary_step(self, x):
        return 1 if x >= 0 else 0

    def forward(self, input_features):
        # Forward pass of the binary step activation function

        # Save the input features for potential use in the backward pass
        self.input_features = input_features
        # Apply the binary step activation element-wise using np.vectorize
        self.activation = np.vectorize(self.binary_step)(self.input_features)

        # Return the result of the activation
        return self.activation
```


writing custom model like pytorch.

mlp.py 
```python
from module import Module
from linear import Linear
from activation import ReLU,Sigmoid, Tanh , BinaryStep
class MLP_XOR(Module):
    # define model elements
    def __init__(self, input_size:float, output_size:float):

        # input to first hidden layer
        self.hidden1 = Linear(input_size, 3)
        self.act1 = ReLU()
        # second hidden layer
        self.hidden2 = Linear(3, 3)
        self.act2 = ReLU()
        # third hidden layer and output
        self.hidden3 = Linear(3, output_size)
        self.act3 = Sigmoid()

    # forward propagate input
    def forward(self, X):
        # input to first hidden layer
        X = self.hidden1.forward(X)
        X = self.act1.forward(X)
        # second hidden layer
        X = self.hidden2.forward(X)
        X = self.act2.forward(X)
        # third hidden layer and output
        X = self.hidden3.forward(X)
        X = self.act3.forward(X)
        return X

```



main.py (run our model)
```python  
import numpy as np
from mlp import MLP_XOR

x_train = np.array([[[0,0]], [[0,1]], [[1,0]], [[1,1]]])
# One-hot encoding for binary classification:
# - [1, 0] represents the class label 0
# - [0, 1] represents the class label 1
y_train = np.array([[[1, 0]],   # Class 0
                   [[0, 1]],    # Class 1
                   [[0, 1]],    # Class 1
                   [[1, 0]]])   # Class 0

def predict(row, model):

    # make prediction
    Y = model.forward(row)
    return Y


model = MLP_XOR(input_size=2 ,output_size=2)

output = predict(x_train, model)

print(output)
```
### output
[[[0.50000000 0.50000000]]\
 [[0.50390420 0.50094783]]\
 [[0.50570128 0.49933700]]\
 [[0.50655580 0.49935189]]]

**Are we doing something wrong? Why is our output like this?** \
**My answer is no**. Till now our MLP model doesn't know how to find patterns inside the logic. 
In simpler terms, the MLP lacks the ability to optimize its weight and bias values in order to produce the desired output. To overcome this limitation, we need to delve into the concept of backward propagation.

## Backward propagation